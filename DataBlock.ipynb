{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Data Bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by @ste & @zachcaceres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "import mimetypes\n",
    "from fastai.vision import *\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "#for jupyter Display\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard path notation for fast.ai\n",
    "# The files willbe saved on $HOME/.fastai/data/timit/\n",
    "path = Path(Path.home()/'.fastai/data/timit')\n",
    "if path.exists: print(f'Working directory: {path}')\n",
    "else: print('Missing data folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Block classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "\n",
    "#Parameters\n",
    "MIN_SAMPLE_SIZE = 201\n",
    "\n",
    "# These are valid file extensions for audio files\n",
    "AUDIO_EXTENSIONS = set(k for k,v in mimetypes.types_map.items() if v.startswith('audio/'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AudioItem\n",
    "This is the base class of out audio data. It contains two basic information about the \"sound\":\n",
    "* sr: the sample rate\n",
    "* data: the actual signal\n",
    "\n",
    "**IMPORTANT:** the audio signal is one-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export        \n",
    "class AudioItem(ItemBase):\n",
    "    def __init__(self, data=None, sr=16000, **kwargs):\n",
    "        self.data = data.reshape(-1) # Always flatten out to single dimension signal!\n",
    "        self.sr = sr\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def __str__(self): return f'Duration: {self.duration} seconds.'\n",
    "    def __len__(self): return self.data.shape[0]\n",
    "    def _repr_html_(self): return f'{self.__str__()}<br />{self.ipy_audio._repr_html_()}'\n",
    "    \n",
    "    def show(self, title:Optional[str]=None, **kwargs):\n",
    "        \"Show sound on `ax` with `title`, using `cmap` if single-channel, overlaid with optional `y`\"\n",
    "        self.hear(title=title)\n",
    "\n",
    "    def hear(self, title=None):\n",
    "        if title is not None: print(title)\n",
    "        display(self.ipy_audio)\n",
    "\n",
    "    def apply_tfms(self, tfms):\n",
    "        for tfm in tfms:\n",
    "            self.data = tfm(self.data)\n",
    "        return self\n",
    "        \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def ipy_audio(self): \n",
    "        return Audio(data=self.data, rate=self.sr)\n",
    "\n",
    "    @property\n",
    "    def duration(self): return len(self.data)/self.sr\n",
    "\n",
    "    @classmethod\n",
    "    def open(cls, fileName, **kwargs):\n",
    "        p = Path(fileName)\n",
    "        if p.exists():\n",
    "            signal,samplerate = torchaudio.load(str(fileName))\n",
    "            return AudioItem(signal,samplerate)\n",
    "        raise f'File not fund: {fileName}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_AudioItem_create_from_data():\n",
    "    signal,samplerate = torchaudio.load(str(path/'TRAIN/DR1/MDPK0/SA1.WAV'))\n",
    "    a = AudioItem(signal,samplerate)\n",
    "    assert 1 == len(a.data.shape), 'Single dimension data'\n",
    "    assert a.data.shape[0] > 100, 'Has data'\n",
    "    assert 16000 == a.sr\n",
    "    display(a)\n",
    "\n",
    "test_AudioItem_create_from_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_AudioItem_create_from_audio_file():\n",
    "    a = AudioItem.open(str(path/'TRAIN/DR1/MDPK0/SA1.WAV'))\n",
    "    assert 1 == len(a.data.shape), 'Single dimension data'\n",
    "    assert a.data.shape[0] > 100, 'Has data'\n",
    "    assert 16000 == a.sr\n",
    "    display(a)\n",
    "    \n",
    "test_AudioItem_create_from_audio_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioDataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "class AudioDataBunch(DataBunch):\n",
    "    def hear_ex(self, rows:int=3, ds_type:DatasetType=DatasetType.Valid, **kwargs):\n",
    "        batch = self.dl(ds_type).dataset[:rows]\n",
    "        self.train_ds.hear_xys(batch.x, batch.y, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AudioList\n",
    "This class is responsible to contain a list of AudioItem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "class AudioList(ItemList):\n",
    "    _bunch = AudioDataBunch\n",
    "    \n",
    "    # TODO: __REPR__    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "            \n",
    "    def get(self, i):\n",
    "        item = self.items[i]\n",
    "        if isinstance(item, str):\n",
    "            return AudioItem.open(item)\n",
    "        if isinstance(item, tuple): #data,sr\n",
    "            return AudioItem(item[0],item[1])\n",
    "        raise 'Format not supported!'\n",
    "    \n",
    "    def reconstruct(self, t:Tensor): return Image(t.transpose(1,2))\n",
    "\n",
    "    def hear_xys(self, xs, ys, **kwargs):\n",
    "        for x, y in zip(xs, ys): x.hear(title=y, **kwargs)\n",
    "            \n",
    "    @classmethod\n",
    "    def from_folder(cls, path:PathOrStr='.', extensions:Collection[str]=None, **kwargs)->ItemList:\n",
    "        extensions = ifnone(extensions, AUDIO_EXTENSIONS)\n",
    "        return super().from_folder(path=path, extensions=extensions, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_AudioList_from_df_file_names():\n",
    "    import glob\n",
    "    #Create Data Frame\n",
    "    df = pd.DataFrame(glob.glob(str(path/'**/*.WAV'), recursive=True)[:10])\n",
    "    df.columns = ['FileName']\n",
    "    display(df.head())\n",
    "\n",
    "    #Crete AudioList\n",
    "    ils = AudioList.from_df(df, path, cols=['FileName'])\n",
    "    \n",
    "    #Test a item\n",
    "    i=5\n",
    "    print(f'FileName: {df.FileName[i]}')\n",
    "    a = ils.get(i)\n",
    "    print(a.data.shape, a.sr)\n",
    "    display(a)\n",
    "\n",
    "test_AudioList_from_df_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sig,srt = torchaudio.load(str(path/'TRAIN/DR1/MDPK0/SA1.WAV'))\n",
    "sig,srt = torchaudio.load('/home/ste/.fastai/data/timit/TRAIN/DR7/MPAR0/SX406.WAV')\n",
    "display(Audio(sig,rate=srt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_AudioList_from_df_data_and_sr():\n",
    "    import glob\n",
    "    #Create Data Frame\n",
    "    df = pd.DataFrame(glob.glob(str(path/'**/*.WAV'), recursive=True)[:10])\n",
    "    df.columns = ['FileName']\n",
    "    df['SampleAndSr']=df['FileName'].apply(lambda n: torchaudio.load(n))\n",
    "    #df = df['tmp'].drop()\n",
    "    \n",
    "    display(df.head())\n",
    "\n",
    "    #Crete AudioList\n",
    "    ils = AudioList.from_df(df, path, cols=['SampleAndSr'])\n",
    "    \n",
    "    #Test a item\n",
    "    i=4\n",
    "    print(f'FileName: {df.FileName[i]}')\n",
    "    a = ils.get(i)\n",
    "    print(a.data.shape, a.sr)\n",
    "    display(a)\n",
    "\n",
    "test_AudioList_from_df_data_and_sr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---- TODO: CONTINUE FROM HERE ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_to_mel(x):\n",
    "    \"\"\"Transform AudioItem to spectrogram\"\"\"\n",
    "    src = x.reshape(1,-1) # expect single sample...\n",
    "    return transforms.MelSpectrogram(\n",
    "        sr=16000,\n",
    "        n_fft=1024,\n",
    "#        hop_length=512, \n",
    "#        n_mels=128,                  \n",
    "#        power=1.0, \n",
    "#        fmin=20, fmax=8000        \n",
    "    )(src)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "librosa.feature.melspectrogram(y, sr=sr, n_fft=1024, \n",
    "                                       hop_length=512, n_mels=128,                  \n",
    "                                      power=1.0, fmin=20, fmax=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tfm_to_mel():\n",
    "    sig,sr = torchaudio.load(str(path/'TRAIN/DR1/MDPK0/SA1.WAV'))\n",
    "    x = AudioItem(sig, sr=sr)\n",
    "    display(x)\n",
    "    img = tfm_to_mel(x)\n",
    "    print(img.shape)\n",
    "    plt.imshow(img[0])\n",
    "    \n",
    "test_tfm_to_mel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max(t, mx=1000, value=0):\n",
    "    \"\"\"Pad tensor with `value` until it reaches length `mx`\"\"\"\n",
    "    if t.shape[1] == mx: return t\n",
    "    return F.pad(t, (0,0, 0,mx-t.shape[1]), value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_phn_file(p_file, sig, sr, delimiter=' '):\n",
    "    df = pd.read_csv(p_file, delimiter=delimiter, header=None)\n",
    "    df.columns = ['Start', 'End', 'Phn']\n",
    "    df['SampleAndSr'] = df.apply(lambda x : (sig[-1][x['Start']: x['End']], sr), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "def create_phn_df(path, count=100):\n",
    "    phns = []\n",
    "    final = pd.DataFrame()\n",
    "\n",
    "    for phn_file in glob.glob(str(path/'**/*.PHN'), recursive=True)[:count]:\n",
    "        sig,sr = torchaudio.load(str(phn_file.replace('PHN', 'WAV')))\n",
    "        df = process_phn_file(phn_file, sig, sr, delimiter=' ')\n",
    "        df['Source'] = phn_file\n",
    "        final = final.append(df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "df = create_phn_df(path/'TRAIN')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.SampleAndSr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_log(x):\n",
    "    '''Fake transformation that logs x shape'''\n",
    "    print(f'Shape of transform input: {x.shape}')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_flatten(x):\n",
    "    return x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal datablock setup from our AudioList from above.\n",
    "data_from_df = (AudioList.from_df(df, path, cols=['SampleAndSr'])\n",
    "    .split_by_rand_pct(0.1, seed=1)\n",
    "    .label_from_df('Phn')\n",
    "    .transform([[tfm_log, tfm_to_mel, pad_to_max, tfm_log], \n",
    "                [tfm_log, tfm_to_mel, pad_to_max, tfm_log]])\n",
    "    .databunch(bs=8))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_df.one_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = AudioList.from_df(df, path, cols=['Sample']); print(type(t0));\n",
    "#t1 = t0.audio_transform([[tfm_log],[tfm_log]]); print(type(t1));\n",
    "t1.get(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Normal datablock setup from our AudioList from above.\n",
    "t0 = AudioList.from_df(df, path, cols=['Sample']); print(type(t0));\n",
    "t1 = t0.split_by_rand_pct(0.1, seed=1); print(type(t1), type(t1.lists[0]));\n",
    "t2 = t1.label_from_df('Phn'); print(type(t2));\n",
    "t3 = t2.transform([[pad_to_max], [pad_to_max]]); print(type(t3));\n",
    "t4 = t3.databunch(bs=8); print(type(t4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_from_df.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_df.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">TODO: merge with Yes/No sample</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ils = AudioList.from_folder(path,extensions=AUDIO_EXTENSIONS)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FIXME\n",
    "ils.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python notebook2script.py DataBlock.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
